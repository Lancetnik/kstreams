{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kstreams kstreams is a library/micro framework to use with kafka . It has simple kafka streams implementation that gives certain guarantees, see below. Requirements python 3.8+ Installation pip install kstreams Usage import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstream\" ) async def consume ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await create_engine . send ( \"local--kstreams\" , value = payload ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 3 ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) Development This repo requires the use of poetry instead of pip. Note : If you want to have the virtualenv in the same path as the project first you should run poetry config --local virtualenvs.in-project true To install the dependencies just execute: poetry install Then you can activate the virtualenv with poetry shell Run test: ./scripts/test Run code linting ( black and isort ) ./scripts/lint Commit messages The use of commitizen is recommended. Commitizen is part of the dev dependencies. cz commit","title":"Introduction"},{"location":"#kstreams","text":"kstreams is a library/micro framework to use with kafka . It has simple kafka streams implementation that gives certain guarantees, see below.","title":"Kstreams"},{"location":"#requirements","text":"python 3.8+","title":"Requirements"},{"location":"#installation","text":"pip install kstreams","title":"Installation"},{"location":"#usage","text":"import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstream\" ) async def consume ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await create_engine . send ( \"local--kstreams\" , value = payload ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 3 ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ())","title":"Usage"},{"location":"#development","text":"This repo requires the use of poetry instead of pip. Note : If you want to have the virtualenv in the same path as the project first you should run poetry config --local virtualenvs.in-project true To install the dependencies just execute: poetry install Then you can activate the virtualenv with poetry shell Run test: ./scripts/test Run code linting ( black and isort ) ./scripts/lint","title":"Development"},{"location":"#commit-messages","text":"The use of commitizen is recommended. Commitizen is part of the dev dependencies. cz commit","title":"Commit messages"},{"location":"configuration/","text":"Settings are read using pkgsettings . Settings Setting Description Default SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS Kafka servers [\"localhost:9092\"] SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL Kafka security protocol PLAINTEXT SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX Topic prefix local-- SERVICE_KSTREAMS_KAFKA_SSL_CERT_DATA client certificate data None SERVICE_KSTREAMS_KAFKA_SSL_KEY_DATA client private key data None SERVICE_KSTREAMS_KAFKA_SSL_CABUNDLE_DATA cabundle data (not needed for cluster environments) None SERVICE_KSTREAMS_KAFKA_SSL_CONTEXT ssl_context None NB: private key data and certificates should NOT be stored in git. Usually the data is retreived from a secret store like Vault. How to use with pkgsettings from pkgsettings import Settings from kstreams.conf import settings as kstreams_settings kstreams_settings . configure ( SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS = [ \"localhost:9092\" ], SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL = \"PLAINTEXT\" , SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX = \"local--\" , )","title":"Configuration"},{"location":"configuration/#settings","text":"Setting Description Default SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS Kafka servers [\"localhost:9092\"] SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL Kafka security protocol PLAINTEXT SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX Topic prefix local-- SERVICE_KSTREAMS_KAFKA_SSL_CERT_DATA client certificate data None SERVICE_KSTREAMS_KAFKA_SSL_KEY_DATA client private key data None SERVICE_KSTREAMS_KAFKA_SSL_CABUNDLE_DATA cabundle data (not needed for cluster environments) None SERVICE_KSTREAMS_KAFKA_SSL_CONTEXT ssl_context None NB: private key data and certificates should NOT be stored in git. Usually the data is retreived from a secret store like Vault.","title":"Settings"},{"location":"configuration/#how-to-use-with-pkgsettings","text":"from pkgsettings import Settings from kstreams.conf import settings as kstreams_settings kstreams_settings . configure ( SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS = [ \"localhost:9092\" ], SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL = \"PLAINTEXT\" , SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX = \"local--\" , )","title":"How to use with pkgsettings"},{"location":"getting_started/","text":"You can starting using kstreams with simple producers and consumers and/or integrated it with any async framework like FastAPI Simple consumer and producer import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) (This script is complete, it should run \"as is\") FastAPI Create the kafka cluster: make kafka-cluster Creation your topics: make create-topic topic-name=local--hello-world # streaming.engine.py from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" , ) # app.py from .streaming.streams import stream_engine from fastapi import FastAPI from starlette.responses import Response from starlette_prometheus import metrics , PrometheusMiddleware def create_app (): app = FastAPI () add_endpoints ( app ) _setup_prometheus ( app ) @app . on_event ( \"startup\" ) async def startup_event (): await stream_engine . start () @app . on_event ( \"shutdown\" ) async def shutdown_event (): await stream_engine . stop () return app def add_endpoints ( app ): @app . get ( \"/events\" ) async def post_produce_event () -> None : payload = { \"message\" : \"hello world!\" } metadata = await stream_engine . send ( \"local--kstream\" , value = payload , ) msg = ( f \"Produced event on topic: { metadata . topic } , \" f \"part: { metadata . partition } , offset: { metadata . offset } \" ) return Response ( msg ) def _setup_prometheus ( app : FastAPI ) -> None : app . add_middleware ( PrometheusMiddleware , filter_unhandled_paths = True ) app . add_api_route ( \"/metrics\" , metrics ) application = create_app () # streaming.streams.py from .engine import stream_engine from kstreams import Stream @stream_engine . stream ( \"local--kstream\" ) async def stream ( stream : Stream ): print ( \"consuming.....\" ) async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . payload } \" )","title":"Getting Started"},{"location":"getting_started/#simple-consumer-and-producer","text":"import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"local--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) (This script is complete, it should run \"as is\")","title":"Simple consumer and producer"},{"location":"getting_started/#fastapi","text":"Create the kafka cluster: make kafka-cluster Creation your topics: make create-topic topic-name=local--hello-world # streaming.engine.py from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" , ) # app.py from .streaming.streams import stream_engine from fastapi import FastAPI from starlette.responses import Response from starlette_prometheus import metrics , PrometheusMiddleware def create_app (): app = FastAPI () add_endpoints ( app ) _setup_prometheus ( app ) @app . on_event ( \"startup\" ) async def startup_event (): await stream_engine . start () @app . on_event ( \"shutdown\" ) async def shutdown_event (): await stream_engine . stop () return app def add_endpoints ( app ): @app . get ( \"/events\" ) async def post_produce_event () -> None : payload = { \"message\" : \"hello world!\" } metadata = await stream_engine . send ( \"local--kstream\" , value = payload , ) msg = ( f \"Produced event on topic: { metadata . topic } , \" f \"part: { metadata . partition } , offset: { metadata . offset } \" ) return Response ( msg ) def _setup_prometheus ( app : FastAPI ) -> None : app . add_middleware ( PrometheusMiddleware , filter_unhandled_paths = True ) app . add_api_route ( \"/metrics\" , metrics ) application = create_app () # streaming.streams.py from .engine import stream_engine from kstreams import Stream @stream_engine . stream ( \"local--kstream\" ) async def stream ( stream : Stream ): print ( \"consuming.....\" ) async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . payload } \" )","title":"FastAPI"},{"location":"metrics/","text":"Metrics are generated by prometheus_client . You must be responsable of setting up a webserver to expose the metrics . Metrics Producer topic_partition_offsets : Gauge of offsets per topic/partition Consumer consumer_committed : Gauge of consumer commited per topic/partition in a consumer group consumer_position : Gauge of consumer current position per topic/partition in a consumer group consumer_highwater : Gauge of consumer highwater per topic/partition in a consumer group consumer_lag : Gauge of current consumer lag per topic/partition in a consumer group","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#producer","text":"topic_partition_offsets : Gauge of offsets per topic/partition","title":"Producer"},{"location":"metrics/#consumer","text":"consumer_committed : Gauge of consumer commited per topic/partition in a consumer group consumer_position : Gauge of consumer current position per topic/partition in a consumer group consumer_highwater : Gauge of consumer highwater per topic/partition in a consumer group consumer_lag : Gauge of current consumer lag per topic/partition in a consumer group","title":"Consumer"},{"location":"serialization/","text":"You can use custom serialization/deserialization . The protocol is the following: from typing import Any , Dict , Optional , Protocol import aiokafka class ValueDeserializer ( Protocol ): async def deserialize ( self , consumer_record : aiokafka . structs . ConsumerRecord , ** kwargs ) -> Any : \"\"\" This method is used to deserialize the data in a KPN way. End users can provide their own class overriding this method. If the engine was created with a schema_store_client, it will be available. class CustomValueDeserializer(ValueDeserializer): async deserialize(self, consumer_record: aiokafka.structs.ConsumerRecord): # custom logic and return something like a ConsumerRecord return consumer_record \"\"\" ... class ValueSerializer ( Protocol ): async def serialize ( self , payload : Any , headers : Optional [ Dict [ str , str ]] = None , value_serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\" Serialize the payload to bytes \"\"\" ... You can write custom Serializers and Deserializers . There are 2 ways of using them: Initialize the engine with them Initilize streams with a deserializer and produce events with serializers stream_engine = create_engine ( title = \"my-stream-engine\" , value_serializer = MyValueSerializer (), value_deserializer = MyValueDeserializer (), ) @stream_engine . stream ( topic , value_deserializer = MyDeserializer ()) async def hello_stream ( stream : Stream ): async for event in stream : save_to_db ( event ) await stream_engine . send ( topic , value = { \"message\" : \"test\" } headers = { \"content-type\" : consts . APPLICATION_JSON ,} key = \"1\" , value_serializer = MySerializer (), )","title":"Serialization"},{"location":"stream/","text":"A Stream in kstreams is an extension of AIOKafkaConsumer Consuming can be done using kstreams.Stream . You only need to decorate a coroutine with @stream_engine.streams . The decorator has the same aiokafka consumer API at initialization, in other words they accept the same args and kwargs that the aiokafka consumer accepts. Stream usage import asyncio from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" ) # here you can add any other AIOKafkaConsumer config, for example auto_offset_reset @stream_engine . stream ( \"local--kstreams\" , group_id = \"de-my-partition\" ) async def stream ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def main (): await stream_engine . start () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) Creating a Stream instance If for any reason you need to create Streams instances directly, you can do it without using the decorator stream_engine.stream . Stream instance import asyncio from aiokafka import structs from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) class MyValueDeserializer : async def deserialize ( self , consumer_record : structs . ConsumerRecord , ** kwargs ): return consumer_record . value . decode () async def stream ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) stream = Stream ( \"local--kstreams\" , name = \"my-stream\" func = stream , # coroutine or async generator value_deserializer = MyValueDeserializer (), ) # add the stream to the engine stream_engine . add_stream ( stram ) async def main (): await stream_engine . start () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) Stream crashing If your stream crashes for any reason, the event consumption will stop meaning that non event will be consumed from the topic . As an end user you are responsable of deciding what to do. In future version approaches like re-try , stream engine stops on stream crash might be introduced. Crashing example import asyncio from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstreams\" , group_id = \"de-my-partition\" ) async def stream ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed. Payload { cr . payload } \" ) async def produce (): await stream_engine . send ( \"local--kstreams\" , value = b \"Hi\" ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) CRASHED Stream!!! Task <Task pending name = 'Task-23' coro = <BaseStream.start.<locals>.func_wrapper () running at /Users/Projects/kstreams/kstreams/streams.py:55>> 'ConsumerRecord' object has no attribute 'payload' Traceback ( most recent call last ) : File \"/Users/Projects/kstreams/kstreams/streams.py\" , line 52 , in func_wrapper await self.func ( self ) File \"/Users/Projects/kstreams/examples/fastapi_example/streaming/streams.py\" , line 9 , in stream print ( f \"Event consumed: headers: {cr.headers}, payload: {cr.payload}\" ) AttributeError: 'ConsumerRecord' object has no attribute 'payload' Consuming from multiple topics Consuming from multiple topics using one stream is possible. A List[str] of topics must be provided. Consume from multiple topics stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ([ \"local--kstreams\" , \"local--hello-world\" ], group_id = \"example-group\" ) async def consume ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed from topic { cr . topic } : headers: { cr . headers } , payload: { cr . value } \" ) Changing consumer behavior Most of the time you will only set the topic and the group_id to the consumer , but sometimes you might want more control over it, for example changing the policy for resetting offsets on OffsetOutOfRange errors or session timeout . To do this, you have to use the same kwargs as the aiokafka consumer API # The consumer sends periodic heartbeats every 500 ms # On OffsetOutOfRange errors, the offset will move to the oldest available message (\u2018earliest\u2019) @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , session_timeout_ms = 500 , auto_offset_reset \"earliest\" ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) Manual commit When processing more sensitive data and you want to be sure that the kafka offeset is commited once that you have done your tasks, you can use enable_auto_commit=False mode of Consumer. Manual commit example @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , enable_auto_commit = False ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) # We need to make sure that the pyalod was stored before commiting the kafka offset await store_in_database ( payload ) await stream . consumer . commit () # You need to commit!!! Note This is a tradeoff from at most once to at least once delivery, to achieve exactly once you will need to save offsets in the destination database and validate those yourself. Yield from stream Sometimes is useful to yield values from a stream so you can consume events in your on phase or because you want to return results to the frontend (SSE example). If you use the yield keyword inside a coroutine it will be \"transform\" to a asynchronous generator function , meaning that inside there is an async generator and it can be consumed. Consuming an async generator is simple, you just use the async for in clause. Because consuming events only happens with the for loop , you have to make sure that the Stream has been started properly and after leaving the async for in the stream has been properly stopped. To facilitate the process, we have context manager that makes sure of the starting/stopping process. Yield example # Create your stream @stream_engine . stream ( \"local--kstream\" ) async def stream ( stream : Stream ): async for cr in stream : yield cr . value # Consume the stream: async with stream as stream_flow : # Use the context manager async for value in stream_flow : ... # do something with value (cr.value) Note If for some reason you interrupt the \"async for in\" in the async generator, the Stream will stopped consuming events meaning that the lag will increase.","title":"Stream"},{"location":"stream/#creating-a-stream-instance","text":"If for any reason you need to create Streams instances directly, you can do it without using the decorator stream_engine.stream . Stream instance import asyncio from aiokafka import structs from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) class MyValueDeserializer : async def deserialize ( self , consumer_record : structs . ConsumerRecord , ** kwargs ): return consumer_record . value . decode () async def stream ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) stream = Stream ( \"local--kstreams\" , name = \"my-stream\" func = stream , # coroutine or async generator value_deserializer = MyValueDeserializer (), ) # add the stream to the engine stream_engine . add_stream ( stram ) async def main (): await stream_engine . start () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ())","title":"Creating a Stream instance"},{"location":"stream/#stream-crashing","text":"If your stream crashes for any reason, the event consumption will stop meaning that non event will be consumed from the topic . As an end user you are responsable of deciding what to do. In future version approaches like re-try , stream engine stops on stream crash might be introduced. Crashing example import asyncio from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"local--kstreams\" , group_id = \"de-my-partition\" ) async def stream ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed. Payload { cr . payload } \" ) async def produce (): await stream_engine . send ( \"local--kstreams\" , value = b \"Hi\" ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) CRASHED Stream!!! Task <Task pending name = 'Task-23' coro = <BaseStream.start.<locals>.func_wrapper () running at /Users/Projects/kstreams/kstreams/streams.py:55>> 'ConsumerRecord' object has no attribute 'payload' Traceback ( most recent call last ) : File \"/Users/Projects/kstreams/kstreams/streams.py\" , line 52 , in func_wrapper await self.func ( self ) File \"/Users/Projects/kstreams/examples/fastapi_example/streaming/streams.py\" , line 9 , in stream print ( f \"Event consumed: headers: {cr.headers}, payload: {cr.payload}\" ) AttributeError: 'ConsumerRecord' object has no attribute 'payload'","title":"Stream crashing"},{"location":"stream/#consuming-from-multiple-topics","text":"Consuming from multiple topics using one stream is possible. A List[str] of topics must be provided. Consume from multiple topics stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ([ \"local--kstreams\" , \"local--hello-world\" ], group_id = \"example-group\" ) async def consume ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed from topic { cr . topic } : headers: { cr . headers } , payload: { cr . value } \" )","title":"Consuming from multiple topics"},{"location":"stream/#changing-consumer-behavior","text":"Most of the time you will only set the topic and the group_id to the consumer , but sometimes you might want more control over it, for example changing the policy for resetting offsets on OffsetOutOfRange errors or session timeout . To do this, you have to use the same kwargs as the aiokafka consumer API # The consumer sends periodic heartbeats every 500 ms # On OffsetOutOfRange errors, the offset will move to the oldest available message (\u2018earliest\u2019) @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , session_timeout_ms = 500 , auto_offset_reset \"earliest\" ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" )","title":"Changing consumer behavior"},{"location":"stream/#manual-commit","text":"When processing more sensitive data and you want to be sure that the kafka offeset is commited once that you have done your tasks, you can use enable_auto_commit=False mode of Consumer. Manual commit example @stream_engine . stream ( \"local--kstream\" , group_id = \"de-my-partition\" , enable_auto_commit = False ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) # We need to make sure that the pyalod was stored before commiting the kafka offset await store_in_database ( payload ) await stream . consumer . commit () # You need to commit!!! Note This is a tradeoff from at most once to at least once delivery, to achieve exactly once you will need to save offsets in the destination database and validate those yourself.","title":"Manual commit"},{"location":"stream/#yield-from-stream","text":"Sometimes is useful to yield values from a stream so you can consume events in your on phase or because you want to return results to the frontend (SSE example). If you use the yield keyword inside a coroutine it will be \"transform\" to a asynchronous generator function , meaning that inside there is an async generator and it can be consumed. Consuming an async generator is simple, you just use the async for in clause. Because consuming events only happens with the for loop , you have to make sure that the Stream has been started properly and after leaving the async for in the stream has been properly stopped. To facilitate the process, we have context manager that makes sure of the starting/stopping process. Yield example # Create your stream @stream_engine . stream ( \"local--kstream\" ) async def stream ( stream : Stream ): async for cr in stream : yield cr . value # Consume the stream: async with stream as stream_flow : # Use the context manager async for value in stream_flow : ... # do something with value (cr.value) Note If for some reason you interrupt the \"async for in\" in the async generator, the Stream will stopped consuming events meaning that the lag will increase.","title":"Yield from stream"},{"location":"test_client/","text":"To test your streams or perform e2e tests you can make use of the test_utils.TestStreamClient . The TestStreamClient you can send events so you won't need a producer Let's assume that you have the following code example: # simple.py from kstreams import create_engine import asyncio topic = \"local--kstreams\" stream_engine = create_engine ( title = \"my-stream-engine\" ) def on_consume ( value ): print ( f \"Value { value } consumed\" ) return value def on_produce ( metadata ): print ( f \"Metadata { metadata } sent\" ) return metadata @stream_engine . stream ( topic , group_id = \"example-group\" ) async def consume ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) on_consume ( value ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for _ in range ( 5 ): metadata = await stream_engine . send ( topic , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) on_produce ( metadata ) async def main (): await stream_engine . start () await produce () await stream_engine . stop () if __name__ == \"__main__\" : asyncio . run ( main ()) Test stream using the TestStreamClient # test_stream.py import pytest from kstreams.test_utils import TestStreamClient @pytest . mark . asyncio async def test_streams_consume_events (): topic = \"local--kstreams\" # Use the same topic as the stream event = b '{\"message\": \"Hello world!\"}' with patch ( \"example.on_consume\" ) as on_consume : async with TestStreamClient () as test_client : metadata = await test_client . send ( topic , value = event , key = \"1\" ) # send the event with the test client current_offset = metadata . offset assert metadata . topic == topic # send another event and check that the offset was incremented metadata = await test_client . send ( topic , value = b '{\"message\": \"Hello world!\"}' , key = \"1\" ) assert metadata . offset == current_offset + 1 # check that the event was consumed on_consume . assert_called () E2E test # test_example.py import pytest from kstreams.test_utils import TestStreamClient from .example import produce @pytest . mark . asyncio async def test_e2e_example (): \"\"\" Test that events are produce by the engine and consumed by the streams \"\"\" with patch ( \"example.on_consume\" ) as on_consume , patch ( \"example.on_produce\" ) as on_produce : async with TestStreamClient (): await produce () on_produce . call_count == 5 on_consume . call_count == 5","title":"Testing"},{"location":"test_client/#test-stream-using-the-teststreamclient","text":"# test_stream.py import pytest from kstreams.test_utils import TestStreamClient @pytest . mark . asyncio async def test_streams_consume_events (): topic = \"local--kstreams\" # Use the same topic as the stream event = b '{\"message\": \"Hello world!\"}' with patch ( \"example.on_consume\" ) as on_consume : async with TestStreamClient () as test_client : metadata = await test_client . send ( topic , value = event , key = \"1\" ) # send the event with the test client current_offset = metadata . offset assert metadata . topic == topic # send another event and check that the offset was incremented metadata = await test_client . send ( topic , value = b '{\"message\": \"Hello world!\"}' , key = \"1\" ) assert metadata . offset == current_offset + 1 # check that the event was consumed on_consume . assert_called ()","title":"Test stream using the TestStreamClient"},{"location":"test_client/#e2e-test","text":"# test_example.py import pytest from kstreams.test_utils import TestStreamClient from .example import produce @pytest . mark . asyncio async def test_e2e_example (): \"\"\" Test that events are produce by the engine and consumed by the streams \"\"\" with patch ( \"example.on_consume\" ) as on_consume , patch ( \"example.on_produce\" ) as on_produce : async with TestStreamClient (): await produce () on_produce . call_count == 5 on_consume . call_count == 5","title":"E2E test"}]}