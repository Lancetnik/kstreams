{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kstreams kstreams is a library/micro framework to use with kafka . It has simple kafka streams implementation that gives certain guarantees, see below. Requirements python 3.8+ API documentation Installation pip install kstreams Usage import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"dev-kpn-des--kstream\" ) async def consume ( stream : Stream ): for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await create_engine . send ( \"dev-kpn-des--kstreams\" , value = payload ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 3 ) async def main (): await stream_engine . init_streaming () await produce () await stream_engine . stop_streaming () if __name__ == \"__main__\" : asyncio . run ( main ()) Development This repo requires the use of poetry instead of pip. Note : If you want to have the virtualenv in the same path as the project first you should run poetry config --local virtualenvs.in-project true To install the dependencies just execute: poetry install Then you can activate the virtualenv with poetry shell Run test: ./scripts/test Run code linting ( black and isort ) ./scripts/lint Commit messages The use of commitizen is recommended. Commitizen is part of the dev dependencies. cz commit","title":"Introduction"},{"location":"#kstreams","text":"kstreams is a library/micro framework to use with kafka . It has simple kafka streams implementation that gives certain guarantees, see below.","title":"Kstreams"},{"location":"#requirements","text":"python 3.8+","title":"Requirements"},{"location":"#api-documentation","text":"","title":"API documentation"},{"location":"#installation","text":"pip install kstreams","title":"Installation"},{"location":"#usage","text":"import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"dev-kpn-des--kstream\" ) async def consume ( stream : Stream ): for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await create_engine . send ( \"dev-kpn-des--kstreams\" , value = payload ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 3 ) async def main (): await stream_engine . init_streaming () await produce () await stream_engine . stop_streaming () if __name__ == \"__main__\" : asyncio . run ( main ())","title":"Usage"},{"location":"#development","text":"This repo requires the use of poetry instead of pip. Note : If you want to have the virtualenv in the same path as the project first you should run poetry config --local virtualenvs.in-project true To install the dependencies just execute: poetry install Then you can activate the virtualenv with poetry shell Run test: ./scripts/test Run code linting ( black and isort ) ./scripts/lint","title":"Development"},{"location":"#commit-messages","text":"The use of commitizen is recommended. Commitizen is part of the dev dependencies. cz commit","title":"Commit messages"},{"location":"configuration/","text":"Settings are read using pkgsettings . Settings Setting Description Default SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS Kafka servers [\"localhost:9092\"] SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL Kafka security protocol PLAINTEXT SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX Topic prefix dev-kpn-des-- SERVICE_KSTREAMS_KAFKA_SSL_CERT_DATA client certificate data None SERVICE_KSTREAMS_KAFKA_SSL_KEY_DATA client private key data None SERVICE_KSTREAMS_KAFKA_SSL_CABUNDLE_DATA cabundle data (not needed for cluster environments) None SERVICE_KSTREAMS_KAFKA_SSL_CONTEXT ssl_context None NB: private key data and certificates should NOT be stored in git. Usually the data is retreived from a secret store like Vault. How to use with pkgsettings from pkgsettings import Settings from kstreams.conf import settings as kstreams_settings kstreams_settings . configure ( SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS = [ \"localhost:9092\" ], SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL = \"PLAINTEXT\" , SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX = \"dev-kpn-des--\" , )","title":"Configuration"},{"location":"configuration/#settings","text":"Setting Description Default SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS Kafka servers [\"localhost:9092\"] SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL Kafka security protocol PLAINTEXT SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX Topic prefix dev-kpn-des-- SERVICE_KSTREAMS_KAFKA_SSL_CERT_DATA client certificate data None SERVICE_KSTREAMS_KAFKA_SSL_KEY_DATA client private key data None SERVICE_KSTREAMS_KAFKA_SSL_CABUNDLE_DATA cabundle data (not needed for cluster environments) None SERVICE_KSTREAMS_KAFKA_SSL_CONTEXT ssl_context None NB: private key data and certificates should NOT be stored in git. Usually the data is retreived from a secret store like Vault.","title":"Settings"},{"location":"configuration/#how-to-use-with-pkgsettings","text":"from pkgsettings import Settings from kstreams.conf import settings as kstreams_settings kstreams_settings . configure ( SERVICE_KSTREAMS_KAFKA_CONFIG_BOOTSTRAP_SERVERS = [ \"localhost:9092\" ], SERVICE_KSTREAMS_KAFKA_CONFIG_SECURITY_PROTOCOL = \"PLAINTEXT\" , SERVICE_KSTREAMS_KAFKA_TOPIC_PREFIX = \"dev-kpn-des--\" , )","title":"How to use with pkgsettings"},{"location":"getting_started/","text":"You can starting using kstreams with simple producers and consumers and/or integrated it with any async framework like FastAPI Simple consumer and producer import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"dev-kpn-des--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( stream : Stream ): for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"dev-kpn-des--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def main (): await stream_engine . init_streaming () await produce () await stream_engine . stop_streaming () if __name__ == \"__main__\" : asyncio . run ( main ()) (This script is complete, it should run \"as is\") FastAPI Create the kafka cluster: make kafka-cluster Creation your topics: make create-topic topic-name=dev-kpn-des--hello-world # streaming.engine.py from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" , ) # app.py from .streaming.streams import stream_engine from fastapi import FastAPI from starlette.responses import Response from starlette_prometheus import metrics , PrometheusMiddleware def create_app (): app = FastAPI () add_endpoints ( app ) _setup_prometheus ( app ) @app . on_event ( \"startup\" ) async def startup_event (): await stream_engine . init_streaming () @app . on_event ( \"shutdown\" ) async def shutdown_event (): await stream_engine . stop_streaming () return app def add_endpoints ( app ): @app . get ( \"/events\" ) async def post_produce_event () -> None : payload = { \"message\" : \"hello world!\" } metadata = await stream_engine . send ( \"dev-kpn-des--kstream\" , value = payload , ) msg = ( f \"Produced event on topic: { metadata . topic } , \" f \"part: { metadata . partition } , offset: { metadata . offset } \" ) return Response ( msg ) def _setup_prometheus ( app : FastAPI ) -> None : app . add_middleware ( PrometheusMiddleware , filter_unhandled_paths = True ) app . add_api_route ( \"/metrics\" , metrics ) application = create_app () # streaming.streams.py from .engine import stream_engine from kstreams import Stream @stream_engine . stream ( \"dev-kpn-des--kstream\" ) async def stream ( stream : Stream ): print ( \"consuming.....\" ) async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . payload } \" )","title":"Getting Started"},{"location":"getting_started/#simple-consumer-and-producer","text":"import asyncio from kstreams import create_engine , Stream stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ( \"dev-kpn-des--py-stream\" , group_id = \"de-my-partition\" ) async def consume ( stream : Stream ): for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { value } \" ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for i in range ( 5 ): metadata = await stream_engine . send ( \"dev-kpn-des--py-streams\" , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) await asyncio . sleep ( 5 ) async def main (): await stream_engine . init_streaming () await produce () await stream_engine . stop_streaming () if __name__ == \"__main__\" : asyncio . run ( main ()) (This script is complete, it should run \"as is\")","title":"Simple consumer and producer"},{"location":"getting_started/#fastapi","text":"Create the kafka cluster: make kafka-cluster Creation your topics: make create-topic topic-name=dev-kpn-des--hello-world # streaming.engine.py from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" , ) # app.py from .streaming.streams import stream_engine from fastapi import FastAPI from starlette.responses import Response from starlette_prometheus import metrics , PrometheusMiddleware def create_app (): app = FastAPI () add_endpoints ( app ) _setup_prometheus ( app ) @app . on_event ( \"startup\" ) async def startup_event (): await stream_engine . init_streaming () @app . on_event ( \"shutdown\" ) async def shutdown_event (): await stream_engine . stop_streaming () return app def add_endpoints ( app ): @app . get ( \"/events\" ) async def post_produce_event () -> None : payload = { \"message\" : \"hello world!\" } metadata = await stream_engine . send ( \"dev-kpn-des--kstream\" , value = payload , ) msg = ( f \"Produced event on topic: { metadata . topic } , \" f \"part: { metadata . partition } , offset: { metadata . offset } \" ) return Response ( msg ) def _setup_prometheus ( app : FastAPI ) -> None : app . add_middleware ( PrometheusMiddleware , filter_unhandled_paths = True ) app . add_api_route ( \"/metrics\" , metrics ) application = create_app () # streaming.streams.py from .engine import stream_engine from kstreams import Stream @stream_engine . stream ( \"dev-kpn-des--kstream\" ) async def stream ( stream : Stream ): print ( \"consuming.....\" ) async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . payload } \" )","title":"FastAPI"},{"location":"metrics/","text":"Metrics are generated by prometheus_client . You must be responsable of setting up a webserver to expose the metrics . Metrics Producer topic_partition_offsets : Gauge of offsets per topic/partition Consumer consumer_committed : Gauge of consumer commited per topic/partition in a consumer group consumer_position : Gauge of consumer current position per topic/partition in a consumer group consumer_highwater : Gauge of consumer highwater per topic/partition in a consumer group consumer_lag : Gauge of current consumer lag per topic/partition in a consumer group","title":"Metrics"},{"location":"metrics/#metrics","text":"","title":"Metrics"},{"location":"metrics/#producer","text":"topic_partition_offsets : Gauge of offsets per topic/partition","title":"Producer"},{"location":"metrics/#consumer","text":"consumer_committed : Gauge of consumer commited per topic/partition in a consumer group consumer_position : Gauge of consumer current position per topic/partition in a consumer group consumer_highwater : Gauge of consumer highwater per topic/partition in a consumer group consumer_lag : Gauge of current consumer lag per topic/partition in a consumer group","title":"Consumer"},{"location":"serialization/","text":"You can use custom serialization/deserialization . The protocol is the following: from typing import Any , Dict , Optional , Protocol import aiokafka class ValueDeserializer ( Protocol ): async def deserialize ( self , consumer_record : aiokafka . structs . ConsumerRecord , ** kwargs ) -> Any : \"\"\" This method is used to deserialize the data in a KPN way. End users can provide their own class overriding this method. If the engine was created with a schema_store_client, it will be available. class CustomValueDeserializer(ValueDeserializer): async deserialize(self, consumer_record: aiokafka.structs.ConsumerRecord): # custom logic and return something like a ConsumerRecord return consumer_record \"\"\" ... class ValueSerializer ( Protocol ): async def serialize ( self , payload : Any , headers : Optional [ Dict [ str , str ]] = None , value_serializer_kwargs : Optional [ Dict ] = None , ) -> bytes : \"\"\" Serialize the payload to bytes \"\"\" ... You can write custom Serializers and Deserializers . There are 2 ways of using them: Initialize the engine with them Initilize streams with a deserializer and produce events with serializers stream_engine = create_engine ( title = \"my-stream-engine\" , value_serializer = MyValueSerializer (), value_deserializer = MyValueDeserializer (), ) @stream_engine . stream ( topic , value_deserializer = MyDeserializer ()) async def hello_stream ( stream : Stream ): async for event in stream : save_to_db ( event ) await stream_engine . send ( topic , value = { \"message\" : \"test\" } headers = { \"content-type\" : consts . APPLICATION_JSON ,} key = \"1\" , value_serializer = MySerializer (), )","title":"Serialization"},{"location":"stream/","text":"A Stream in kstreams is an extension of AIOKafkaConsumer Consuming can be done using kstreams.Stream . You only need to decorate a coroutine with @stream_engine.streams . The decorator has the same aiokafka consumer API at initialization, in other words they accept the same args and kwargs that the aiokafka consumer accepts. Stream usage import asyncio from kstreams import create_engine stream_engine = create_engine ( title = \"my-stream-engine\" ) # here you can add any other AIOKafkaConsumer config, for example auto_offset_reset @stream_engine . stream ( \"dev-kpn-des--py-stream\" , group_id = \"de-my-partition\" ) async def stream ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) async def main (): await stream_engine . init_streaming () await stream_engine . stop_streaming () if __name__ == \"__main__\" : asyncio . run ( main ()) Consuming from multiple topics Consuming from multiple topics using one stream is possible. A List[str] of topics must be provided. Consume from multiple topics stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ([ \"dev-kpn-des--kstreams\" , \"dev-kpn-des--hello-world\" ], group_id = \"example-group\" ) async def consume ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed from topic { cr . topic } : headers: { cr . headers } , payload: { cr . value } \" ) Changing consumer behavior Most of the time you will only set the topic and the group_id to the consumer , but sometimes you might want more control over it, for example changing the policy for resetting offsets on OffsetOutOfRange errors or session timeout . To do this, you have to use the same kwargs as the aiokafka consumer API # The consumer sends periodic heartbeats every 500 ms # On OffsetOutOfRange errors, the offset will move to the oldest available message (\u2018earliest\u2019) @stream_engine . stream ( \"dev-kpn-des--kstream\" , group_id = \"de-my-partition\" , session_timeout_ms = 500 , auto_offset_reset \"earliest\" ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) Manual commit When processing more sensitive data and you want to be sure that the kafka offeset is commited once that you have done your tasks, you can use enable_auto_commit=False mode of Consumer. Manual commit example @stream_engine . stream ( \"dev-kpn-des--kstream\" , group_id = \"de-my-partition\" , enable_auto_commit = False ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) # We need to make sure that the pyalod was stored before commiting the kafka offset await store_in_database ( payload ) await stream . consumer . commit () # You need to commit!!! Note This is a tradeoff from at most once to at least once delivery, to achieve exactly once you will need to save offsets in the destination database and validate those yourself.","title":"Stream"},{"location":"stream/#consuming-from-multiple-topics","text":"Consuming from multiple topics using one stream is possible. A List[str] of topics must be provided. Consume from multiple topics stream_engine = create_engine ( title = \"my-stream-engine\" ) @stream_engine . stream ([ \"dev-kpn-des--kstreams\" , \"dev-kpn-des--hello-world\" ], group_id = \"example-group\" ) async def consume ( stream : Stream ) -> None : async for cr in stream : print ( f \"Event consumed from topic { cr . topic } : headers: { cr . headers } , payload: { cr . value } \" )","title":"Consuming from multiple topics"},{"location":"stream/#changing-consumer-behavior","text":"Most of the time you will only set the topic and the group_id to the consumer , but sometimes you might want more control over it, for example changing the policy for resetting offsets on OffsetOutOfRange errors or session timeout . To do this, you have to use the same kwargs as the aiokafka consumer API # The consumer sends periodic heartbeats every 500 ms # On OffsetOutOfRange errors, the offset will move to the oldest available message (\u2018earliest\u2019) @stream_engine . stream ( \"dev-kpn-des--kstream\" , group_id = \"de-my-partition\" , session_timeout_ms = 500 , auto_offset_reset \"earliest\" ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" )","title":"Changing consumer behavior"},{"location":"stream/#manual-commit","text":"When processing more sensitive data and you want to be sure that the kafka offeset is commited once that you have done your tasks, you can use enable_auto_commit=False mode of Consumer. Manual commit example @stream_engine . stream ( \"dev-kpn-des--kstream\" , group_id = \"de-my-partition\" , enable_auto_commit = False ) async def stream ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) # We need to make sure that the pyalod was stored before commiting the kafka offset await store_in_database ( payload ) await stream . consumer . commit () # You need to commit!!! Note This is a tradeoff from at most once to at least once delivery, to achieve exactly once you will need to save offsets in the destination database and validate those yourself.","title":"Manual commit"},{"location":"test_client/","text":"To test your streams or perform e2e tests you can make use of the test_utils.TestStreamClient . The TestStreamClient you can send events so you won't need a producer Let's assume that you have the following code example: # simple.py from kstreams import create_engine import asyncio topic = \"dev-kpn-des--kstreams\" stream_engine = create_engine ( title = \"my-stream-engine\" ) def on_consume ( value ): print ( f \"Value { value } consumed\" ) return value def on_produce ( metadata ): print ( f \"Metadata { metadata } sent\" ) return metadata @stream_engine . stream ( topic , group_id = \"example-group\" ) async def consume ( stream : Stream ): async for cr in stream : print ( f \"Event consumed: headers: { cr . headers } , payload: { cr . value } \" ) on_consume ( value ) async def produce (): payload = b '{\"message\": \"Hello world!\"}' for _ in range ( 5 ): metadata = await stream_engine . send ( topic , value = payload , key = \"1\" ) print ( f \"Message sent: { metadata } \" ) on_produce ( metadata ) async def main (): await stream_engine . init_streaming () await produce () await stream_engine . stop_streaming () if __name__ == \"__main__\" : asyncio . run ( main ()) Test stream using the TestStreamClient # test_stream.py import pytest from kstreams.test_utils import TestStreamClient @pytest . mark . asyncio async def test_streams_consume_events (): topic = \"dev-kpn-des--kstreams\" # Use the same topic as the stream event = b '{\"message\": \"Hello world!\"}' with patch ( \"example.on_consume\" ) as on_consume : async with TestStreamClient () as test_client : metadata = await test_client . send ( topic , value = event , key = \"1\" ) # send the event with the test client current_offset = metadata . offset assert metadata . topic == topic # send another event and check that the offset was incremented metadata = await test_client . send ( topic , value = b '{\"message\": \"Hello world!\"}' , key = \"1\" ) assert metadata . offset == current_offset + 1 # check that the event was consumed on_consume . assert_called () E2E test # test_example.py import pytest from kstreams.test_utils import TestStreamClient from .example import produce @pytest . mark . asyncio async def test_e2e_example (): \"\"\" Test that events are produce by the engine and consumed by the streams \"\"\" with patch ( \"example.on_consume\" ) as on_consume , patch ( \"example.on_produce\" ) as on_produce : async with TestStreamClient (): await produce () on_produce . call_count == 5 on_consume . call_count == 5","title":"Testing"},{"location":"test_client/#test-stream-using-the-teststreamclient","text":"# test_stream.py import pytest from kstreams.test_utils import TestStreamClient @pytest . mark . asyncio async def test_streams_consume_events (): topic = \"dev-kpn-des--kstreams\" # Use the same topic as the stream event = b '{\"message\": \"Hello world!\"}' with patch ( \"example.on_consume\" ) as on_consume : async with TestStreamClient () as test_client : metadata = await test_client . send ( topic , value = event , key = \"1\" ) # send the event with the test client current_offset = metadata . offset assert metadata . topic == topic # send another event and check that the offset was incremented metadata = await test_client . send ( topic , value = b '{\"message\": \"Hello world!\"}' , key = \"1\" ) assert metadata . offset == current_offset + 1 # check that the event was consumed on_consume . assert_called ()","title":"Test stream using the TestStreamClient"},{"location":"test_client/#e2e-test","text":"# test_example.py import pytest from kstreams.test_utils import TestStreamClient from .example import produce @pytest . mark . asyncio async def test_e2e_example (): \"\"\" Test that events are produce by the engine and consumed by the streams \"\"\" with patch ( \"example.on_consume\" ) as on_consume , patch ( \"example.on_produce\" ) as on_produce : async with TestStreamClient (): await produce () on_produce . call_count == 5 on_consume . call_count == 5","title":"E2E test"}]}